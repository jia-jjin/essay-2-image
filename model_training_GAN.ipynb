{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a13706b-3364-44cb-942c-212732b1461a",
   "metadata": {},
   "source": [
    "# GAN Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27467308-1c3f-4d67-bf97-9b60043f8c9e",
   "metadata": {},
   "source": [
    "GANs are used for generating words that might match up to the input provided, for example for the input \"I am feeling\", the model might give the output \"I am feeling very great today!\" as a result. It gives the input based on the patterns obtained from the training datasets, which might not be very applicable to what I am trying to aim for right now. Nonetheless, I still gave it a try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46cfa2-368b-4b40-bc7f-0ab02a3429cd",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9070e665-d99b-4ddd-93c6-7c580067745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import string\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77568a1a-fe0c-4107-958a-b1181b96e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cleaned_training_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76536c5e-338d-45b7-87a6-7c7ccc3b921d",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1a66e5-d762-45ec-a410-95cd41f3858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words: 678114\n",
      "Unique Words: 16515\n"
     ]
    }
   ],
   "source": [
    "# text = ''.join([c for c in ' '.join(df.dropna().values.flatten()).lower() if c not in string.punctuation])\n",
    "text = ' '.join([str(sentence).strip() if str(sentence).strip()[-1] == '.' else str(sentence).strip() + '.' for sentence in df['prompts'].values.flatten()]).lower()\n",
    "words = word_tokenize(text)\n",
    "n_words = len(words)\n",
    "unique_words = len(set(words))\n",
    "\n",
    "print('Total Words: %d' % n_words)\n",
    "print('Unique Words: %d' % unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15c08cb-1f18-4393-977e-426af6019f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '$': 4,\n",
       " \"'\": 5,\n",
       " '(': 6,\n",
       " ')': 7,\n",
       " '*': 8,\n",
       " '+': 9,\n",
       " ',': 10,\n",
       " '-': 11,\n",
       " '.': 12,\n",
       " '/': 13,\n",
       " '0': 14,\n",
       " '1': 15,\n",
       " '2': 16,\n",
       " '3': 17,\n",
       " '4': 18,\n",
       " '5': 19,\n",
       " '6': 20,\n",
       " '7': 21,\n",
       " '8': 22,\n",
       " '9': 23,\n",
       " ':': 24,\n",
       " ';': 25,\n",
       " '?': 26,\n",
       " '_': 27,\n",
       " 'a': 28,\n",
       " 'b': 29,\n",
       " 'c': 30,\n",
       " 'd': 31,\n",
       " 'e': 32,\n",
       " 'f': 33,\n",
       " 'g': 34,\n",
       " 'h': 35,\n",
       " 'i': 36,\n",
       " 'j': 37,\n",
       " 'k': 38,\n",
       " 'l': 39,\n",
       " 'm': 40,\n",
       " 'n': 41,\n",
       " 'o': 42,\n",
       " 'p': 43,\n",
       " 'q': 44,\n",
       " 'r': 45,\n",
       " 's': 46,\n",
       " 't': 47,\n",
       " 'u': 48,\n",
       " 'v': 49,\n",
       " 'w': 50,\n",
       " 'x': 51,\n",
       " 'y': 52,\n",
       " 'z': 53,\n",
       " '|': 54,\n",
       " 'à': 55,\n",
       " 'á': 56,\n",
       " 'ç': 57,\n",
       " 'è': 58,\n",
       " 'é': 59,\n",
       " 'í': 60,\n",
       " 'ð': 61,\n",
       " 'ñ': 62,\n",
       " 'ó': 63,\n",
       " 'ô': 64,\n",
       " 'ù': 65,\n",
       " 'ú': 66,\n",
       " 'ł': 67,\n",
       " 'ń': 68,\n",
       " 'š': 69,\n",
       " 'ž': 70,\n",
       " 'क': 71,\n",
       " 'च': 72,\n",
       " 'प': 73,\n",
       " 'म': 74,\n",
       " 'र': 75,\n",
       " 'व': 76,\n",
       " 'ी': 77,\n",
       " '्': 78,\n",
       " '–': 79,\n",
       " '—': 80,\n",
       " '‘': 81,\n",
       " '’': 82,\n",
       " '“': 83,\n",
       " '”': 84,\n",
       " '•': 85,\n",
       " '→': 86,\n",
       " '道': 87}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chars = sorted(list(set(text)))\n",
    "char_to_index = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "char_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecbec10-d437-4e62-8a5f-0594a5ae1890",
   "metadata": {},
   "source": [
    "## Preparing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d6f106-eb9b-4d41-9ecd-86fea05d61a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = []\n",
    "output_words = []\n",
    "input_seq_length = 40\n",
    "\n",
    "for i in range(0, len(text) - input_seq_length , 1):\n",
    "    in_seq = text[i:i + input_seq_length]\n",
    "    out_seq = text[i + input_seq_length]\n",
    "    input_sequence.append([char_to_index[word] for word in in_seq])\n",
    "    output_words.append(char_to_index[out_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f1bdd2b-6777-47fb-8b30-be24478414f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
    "X = X / float(len(unique_chars))\n",
    "\n",
    "y = to_categorical(output_words, num_classes=len(unique_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89b14bbf-85a9-4faf-b621-6bed0773a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=.2,\n",
    "    random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b63eeb-26cb-4523-9d09-1854ce243ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2952565, 40, 1)\n",
      "y_train shape: (2952565, 88)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6549d19-0197-4bce-998a-9b9833ec9793",
   "metadata": {},
   "source": [
    "## Initializing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e32f4e-d96d-4c9a-9c5e-4080d2a830e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">11,352</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m66,560\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m131,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m88\u001b[0m)                  │          \u001b[38;5;34m11,352\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">209,496</span> (818.34 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m209,496\u001b[0m (818.34 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">209,496</span> (818.34 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m209,496\u001b[0m (818.34 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input((input_seq_length, 1)),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    LSTM(128),\n",
    "    Dense(len(unique_chars), activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df1416-f030-4363-83b8-e6431795a386",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db10132-bca4-4bae-8f2c-898f2266cdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m827s\u001b[0m 286ms/step - loss: 2.9440\n",
      "Epoch 2/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m808s\u001b[0m 280ms/step - loss: 2.4920\n",
      "Epoch 3/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m807s\u001b[0m 280ms/step - loss: 2.2558\n",
      "Epoch 4/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m808s\u001b[0m 280ms/step - loss: 2.1045\n",
      "Epoch 5/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m808s\u001b[0m 280ms/step - loss: 1.9972\n",
      "Epoch 6/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m826s\u001b[0m 286ms/step - loss: 1.9119\n",
      "Epoch 7/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m874s\u001b[0m 303ms/step - loss: 1.8439\n",
      "Epoch 8/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m856s\u001b[0m 297ms/step - loss: 1.7887\n",
      "Epoch 9/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m827s\u001b[0m 287ms/step - loss: 1.7421\n",
      "Epoch 10/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m819s\u001b[0m 284ms/step - loss: 1.7037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x29bbc157d10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=1024, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0791ee66-aa78-48ff-84f9-5cd79ba69338",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5680912-329c-478f-875e-8659906b0890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manship permeating the space. the composition is a medi-muered seene in the soft and a soft glow of a soft glgwsent and a soatle eroendent strles and a soatle eroendent of a woung aod a soit groe and a soft glow of a lodern soited sarterns \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Pick a random starting sequence\n",
    "start_index = random.randint(0, len(text) - input_seq_length - 1)\n",
    "seed_text = text[start_index:start_index + input_seq_length]\n",
    "\n",
    "# Generate characters\n",
    "generated_text = seed_text\n",
    "for _ in range(200):  # Generate 200 characters\n",
    "    input_seq = np.array([[char_to_index[char] for char in seed_text]]).reshape(1, input_seq_length, 1)\n",
    "    input_seq = input_seq / float(len(unique_chars))\n",
    "\n",
    "    # Predict the next character\n",
    "    predicted_index = np.argmax(model.predict(input_seq, verbose=0))\n",
    "    next_char = unique_chars[predicted_index]\n",
    "\n",
    "    # Append to generated text and update seed\n",
    "    generated_text += next_char\n",
    "    seed_text = seed_text[1:] + next_char\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd80ce-bccc-4a59-9675-3f666215cc27",
   "metadata": {},
   "source": [
    "As we can see here, this is not really what we're looking for right now, instead it should have a specific pattern based on the whole context of the sentence, and that also means it should generate a new sentence with the same meaning but with different tones or aspect instead, and GANs don't do that quite well based on my understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca105023-4e51-4d7f-8df4-81c951333d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
